{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Task: \n",
    "##  Sentiment Analysis using Traditional Machine Learning\n",
    "-    This project is focused on interpreting and classifying emotions expressed in text data using Natural Language Processing (NLP) . The approach leverages traditional machine learning techniques to analyze and predict sentiments expressed in customer reviews.\n",
    "\n",
    "-   Objectives\n",
    "    -   Understanding Sentiment Distribution: Our first step involves exploring the distribution of sentiments in the dataset. We aim to understand the proportion of positive, neutral, and negative sentiments expressed in customer reviews.\n",
    "    -   Data Preprocessing: This includes cleaning text data, transforming it into a suitable format for analysis (using techniques like TF-IDF), and addressing class imbalances.\n",
    "    -   Model Development and Evaluation: We train and evaluate multiple machine learning models, including Logistic Regression, Random Forest, and Naive Bayes. These models are chosen for their robustness and effectiveness in classification tasks.\n",
    "    -   Performance Analysis: Each model's performance is assessed using metrics like accuracy, precision, recall, and F1-score. Additionally, confusion matrices are utilized for a more detailed evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Pandas to show all columns\n",
    "# Setting pandas display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the already preprocessed review dataset \n",
    "file_path = \"C:/Users/Oby/Desktop/Data Science Portfolio/lemmatized_data.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Visualise the Data's Size, Shape, and Structure to gain a fundamental understanding of the dataset, explore its size, shape, and overall structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing values in the dataset\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'review_title' or 'review_text' is missing\n",
    "data.dropna(subset=['review_title', 'review_text'], inplace=True)\n",
    "\n",
    "# Check for missing values again\n",
    "print(data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Visualization of Ratings Distribution\n",
    "This is to visually represent the distribution of ratings in the review dataset. Understanding how ratings are distributed is crucial for gaining insights into overall customer satisfaction and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize a pie chart to illustrate the proportion of each rating\n",
    "#  Get the value counts of the 'rating' column\n",
    "rating_counts = data['rating'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Add a title\n",
    "plt.title('Distribution of Ratings')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Categorize ratings into 'Good', 'Neutral', and 'Poor' sentiment categories based on the value of the 'rating' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to categorize ratings\n",
    "def categorize_rating(rating):\n",
    "    if rating >= 4:\n",
    "        return 'Good'\n",
    "    elif rating == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Poor'\n",
    "\n",
    "# Apply the function to the 'rating' column\n",
    "data['sentiment'] = data['rating'].apply(categorize_rating)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Graphically represent the distribution of sentiments within the dataset. By visualizing the sentiments as 'Good', 'Neutral', and 'Poor'to gain a clearer understanding of the overall sentiment trends present in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilize a pie chart to show the proportion of each sentiment category within the dataset. \n",
    "# Get the value counts of the 'sentiment' column\n",
    "sentiment_counts = data['sentiment'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Add a title\n",
    "plt.title('Distribution of Sentiments')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Import the necessary libraries for the Mchine learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Converting the categorical sentiment labels in our dataset to numerical form, to ensure the dataset is suitable for the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'sentiment' column\n",
    "data['sentiment_encoded'] = label_encoder.fit_transform(data['sentiment'])\n",
    "\n",
    "#display the value count of the encoded column\n",
    "data[\"sentiment_encoded\"].value_counts()\n",
    "\n",
    "# Good 0, poor(664) 2 and neutral (67) 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Transform the text data from the 'review_text' column into a numerical format using the Term Frequency-Inverse Document Frequency (TF-IDF) method. \n",
    "-   This transformation is crucial for text analysis and machine learning tasks as it converts text into a format that algorithms can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer:\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Apply the vectorizer to the review_text column.\n",
    "X = tfidf_vectorizer.fit_transform(data['review_text'])\n",
    "\n",
    "# Check the matrix shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the target variable\n",
    "y = data[\"sentiment_encoded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Data Imbalance\n",
    "-   Addressing the imbalance in our dataset's target classes is crucial for improving the performance of machine learning models. We employ ADASYN (Adaptive Synthetic Sampling Approach) to oversample the minority classes and achieve a more balanced class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ADASYN object\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Apply ADASYN to generate the oversampled dataset\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
    "\n",
    "# Check the new class distribution (optional)\n",
    "from collections import Counter\n",
    "print(\"Original class distribution:\", Counter(y))\n",
    "print(\"Resampled class distribution:\", Counter(y_resampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-    Splitting the Dataset into Training and Testing Sets to address class imbalance which is  essential for training machine learning models and subsequently evaluating their performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the training and testing sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create baseline models using Logistic Regression, Random Forest, and Naive Bayes\n",
    "-    Focus on three widely-used machine learning algorithms: Logistic Regression, Random Forest, and Naive Bayes. Each of these models brings unique strengths and characteristics, making them suitable for a broad range of classification tasks, including our scenario of sentiment analysis.\n",
    "\n",
    "Why These Models?\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "-   A simple yet powerful linear model for binary and multiclass classification problems.\n",
    "-   Efficient and interpretable, often serving as the first model to try in classification tasks.\n",
    "-   Performs well when the dataset is linearly separable.\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "-   An ensemble learning method based on decision tree classifiers.\n",
    "-   Offers high accuracy through bagging and feature randomness when splitting nodes.\n",
    "-   Handles large data with higher dimensionality well and provides estimates of feature importance.\n",
    "\n",
    "Naive Bayes:\n",
    "\n",
    "-   Based on Bayes' theorem with the assumption of independence between predictors.\n",
    "-   Particularly useful for text classification problems like spam filtering and sentiment analysis.\n",
    "-   Fast and efficient with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   initialize three different machine learning algorithms: Logistic Regression, Random Forest, and Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise Models\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "nb = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Fit each model on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   utilize a Python dictionary to pair model names with their respective initialized instances. Then, by iterating through this dictionary, we can efficiently train each model in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models in a dictionary\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "# Train each model in a loop\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"{name} has been trained.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Evaluate and Visualize Model Performance after training the Logistic Regression, Random Forest, and Naive Bayes models. This step involves making predictions on the test set, calculating accuracy, generating classification reports, and displaying confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy:\", accuracy)\n",
    "    print(f\"\\n{name} Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
